
Otsu's algorithm
    My current understanding is that it finds the point that is the brightess and the point that is the darkerst based on computer logic
    The range of brightness to darkness is reffered to as Intesnsity. Based on intensity it differentiated into the background and foreground
    After the background and foreground values are determined, it runs an algorithm that checks if somethings is closer to being the background or the foreground


Page Analysis
    Page layout analysis, one of the first steps of OCR, divides an image into areas
    of text and non-text, as well as splitting multi-column text into columns [7].
    The page layout analysis in Tesseract is based on the idea of detecting
    tab-stops in a document image. It has the following steps[7]:
    * used the Leptonica library to vertical lines and images
        basically based on the the areas of darkness and brightness, it tries to find lines for columns and rows


Baseline Fitting and Word Detection
    The median height is used approximate the text size in the region
    and components (blobs) that are smaller than some fraction of the
    median height mostly being punctuation, diacritical marks and noise
    are filtered out.

    The blobs are sorted (into ascending order) using x-coordinate (of the
    left edge) as the sort key. This sort makes it possible to track the skew
    across the page.

    Tesseract does word detection by measuring gaps in a limited vertical range
    between the baseline and mean line[4]. Spaces that are close to the threshold
    at this stage are made fuzzy, so that a final decision can be made after word
    recognition.

Word Recognition
    basically it tries to find the spaces between character and cut them at that point (for fix pitched: evenly spaced letters)

    proportional spaceing: Non-fixed-pitch or proportional text spacing is a
                            highly non-trivial task. Fig. 3 illustrates some typical
                            problems. The gap between the tens and units of
                            ‘11.9%’ is a similar size to the general space, and is
                            certainly larger than the kerned space between ‘erated’
                            and ‘junk’. There is no horizontal gap at all between
                            the bounding boxes of ‘of’ and ‘financial’. Tesseract
                            solves most of these problems by measuring gaps in a
                            limited vertical range between the baseline and mean
                            line. Spaces that are close to the threshold at this stage
                            are made fuzzy, so that a final decision can be made
                            after word recognition. 


Character Seperater
    Tesseract attempts to improve the result
    by chopping the blob with worst confidence from the
    character classifier. Candidate chop points are found
    from concave vertices of a polygonal approximation
    [2] of the outline, and may have either another concave
    vertex opposite, or a line segment. It may take up to 3
    pairs of chop points to successfully separate joined
    characters from the ASCII set. 
        * basically it tries to cut seperate words that are too close based on where it detects curves
        * if that doesn't work they will then try to reconnect some characters based ona a value I do not fully understand (A star)

Character classifier
    Static Classification
        basically tesseract converts the characters into simpler polygonal shapes and then basically runs them through their model which has a library of shapes
        To try to make it faster " It uses a method similar to Locality Sensitive Hashing (LSH) to create a short list of possible letter matches for each shape."
        Tesseract then refines these potential matches by measuring the "distance" between the new shape and the prototypes. This distance is calculated by
        Final Decision: Once Tesseract has calculated distances for potential matches, it uses the K-Nearest Neighbor (KNN) method to pick the best-matching character from the shortlist.

    Adaptive Classification
        basically it tries to learn the font/style of the text using the static classifier and then run the OCR again with this knowledge
        
    